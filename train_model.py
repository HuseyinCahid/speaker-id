"""
KonuÅŸmacÄ± tanÄ±ma modeli eÄŸitim scripti.
FarklÄ± ML algoritmalarÄ± ile MFCC Ã¶zellikleri Ã¼zerinde eÄŸitim yapar.
Desteklenen modeller: SVM, Random Forest, Neural Network, AdaBoost
"""
import sys
import os
import argparse
from pathlib import Path

# Add backend directory to Python path
SCRIPT_DIR = Path(__file__).resolve().parent
BACKEND_DIR = SCRIPT_DIR / 'backend'
sys.path.insert(0, str(BACKEND_DIR))

# Windows encoding fix
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from pathlib import Path
import numpy as np
import pickle
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import (
    train_test_split, 
    StratifiedKFold, 
    cross_val_score,
    GridSearchCV,
    RandomizedSearchCV
)
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
from audio_processor import AudioProcessor  # type: ignore

def create_model(model_type: str, random_state: int = 42):
    """
    Model oluÅŸtur.
    
    Args:
        model_type: Model tipi ('svm', 'random_forest', 'neural_network', 'adaboost')
        random_state: Rastgelelik durumu
        
    Returns:
        EÄŸitilmemiÅŸ model
    """
    if model_type == 'svm':
        return SVC(kernel='rbf', probability=True, random_state=random_state)
    elif model_type == 'random_forest':
        return RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            random_state=random_state,
            n_jobs=-1
        )
    elif model_type == 'neural_network':
        return MLPClassifier(
            hidden_layer_sizes=(128, 64),
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size='auto',
            learning_rate='constant',
            learning_rate_init=0.001,
            max_iter=500,
            random_state=random_state,
            early_stopping=True,
            validation_fraction=0.1
        )
    elif model_type == 'adaboost':
        return AdaBoostClassifier(
            n_estimators=50,
            learning_rate=1.0,
            random_state=random_state
        )
    else:
        raise ValueError(f"Bilinmeyen model tipi: {model_type}")


def get_model_filename(model_type: str, feature_type: str = 'mfcc') -> str:
    """Model dosya adÄ±nÄ± dÃ¶ndÃ¼r (sadece MFCC kullanÄ±lÄ±yor)."""
    base_names = {
        'svm': 'svm',
        'random_forest': 'random_forest',
        'neural_network': 'neural_network',
        'adaboost': 'adaboost'
    }
    base_name = base_names.get(model_type, 'model')
    return f'{base_name}_speaker_model.pkl'


def get_hyperparameter_grid(model_type: str):
    """
    Her model tipi iÃ§in hyperparameter grid dÃ¶ndÃ¼r.
    
    Args:
        model_type: Model tipi
        
    Returns:
        Hyperparameter grid dictionary
    """
    if model_type == 'svm':
        return {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
            'kernel': ['rbf', 'poly', 'sigmoid']
        }
    elif model_type == 'random_forest':
        return {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
    elif model_type == 'neural_network':
        return {
            'hidden_layer_sizes': [(64,), (128,), (128, 64), (256, 128)],
            'alpha': [0.0001, 0.001, 0.01],
            'learning_rate_init': [0.0001, 0.001, 0.01],
            'activation': ['relu', 'tanh']
        }
    elif model_type == 'adaboost':
        return {
            'n_estimators': [25, 50, 100],
            'learning_rate': [0.5, 1.0, 1.5, 2.0]
        }
    else:
        return {}


def perform_cross_validation(model, X, y, cv_folds: int = 5):
    """
    Cross-validation performansÄ±nÄ± hesapla.
    
    Args:
        model: EÄŸitilmemiÅŸ model
        X: Ã–zellik matrisi
        y: Etiket vektÃ¶rÃ¼
        cv_folds: Cross-validation fold sayÄ±sÄ±
        
    Returns:
        CV skorlarÄ± ve ortalama/std
    """
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)
    
    return {
        'cv_scores': cv_scores.tolist(),
        'cv_mean': float(np.mean(cv_scores)),
        'cv_std': float(np.std(cv_scores)),
        'cv_folds': cv_folds
    }


def train_speaker_model(
    model_type: str = 'svm', 
    feature_type: str = 'mfcc',
    use_cv: bool = False,
    cv_folds: int = 5,
    use_tuning: bool = False,
    tuning_method: str = 'grid',
    n_iter: int = 20
):
    """
    Ana eÄŸitim fonksiyonu.
    
    Args:
        model_type: Model tipi ('svm', 'random_forest', 'neural_network', 'adaboost')
        feature_type: Ã–zellik tipi ('mfcc' - Mel desteÄŸi kaldÄ±rÄ±ldÄ±)
        use_cv: Cross-validation kullan (default: False)
        cv_folds: Cross-validation fold sayÄ±sÄ± (default: 5)
        use_tuning: Hyperparameter tuning kullan (default: False)
        tuning_method: Tuning yÃ¶ntemi ('grid' veya 'random', default: 'grid')
        n_iter: RandomizedSearchCV iÃ§in iterasyon sayÄ±sÄ± (default: 20)
    """
    model_names = {
        'svm': 'SVM (Support Vector Machine)',
        'random_forest': 'Random Forest',
        'neural_network': 'Neural Network (MLP)',
        'adaboost': 'AdaBoost'
    }
    
    feature_names = {
        'mfcc': 'MFCC (Mel-Frequency Cepstral Coefficients)',
        'mel': 'Mel-Spectrogram'
    }
    
    # Validate feature type (only MFCC supported)
    if feature_type not in ['mfcc']:
        print(f"âš ï¸  Warning: feature_type '{feature_type}' not supported. Using 'mfcc' instead.")
        feature_type = 'mfcc'
    
    print("ğŸ¤ Speaker Identification Model Training")
    print("=" * 50)
    print(f"ğŸ“¦ Model Tipi: {model_names.get(model_type, model_type)}")
    print(f"ğŸµ Ã–zellik Tipi: {feature_names.get(feature_type, feature_type)}")
    if use_cv:
        print(f"ğŸ”„ Cross-Validation: âœ… ({cv_folds} folds)")
    else:
        print(f"ğŸ”„ Cross-Validation: âŒ")
    if use_tuning:
        print(f"ğŸ¯ Hyperparameter Tuning: âœ… ({tuning_method})")
    else:
        print(f"ğŸ¯ Hyperparameter Tuning: âŒ")
    print("=" * 50)
    
    # Yollar
    data_dir = Path("data/raw")
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)
    
    # Audio processor
    processor = AudioProcessor()
    
    # Veri yÃ¼kleme
    print("\nğŸ“‚ Loading audio files...")
    features_list = []
    labels_list = []
    
    if not data_dir.exists():
        print(f"âŒ Error: {data_dir} directory not found!")
        print("\nPlease create the following structure:")
        print("data/raw/")
        print("  speaker_01/")
        print("    utt_0001.wav")
        print("    utt_0002.wav")
        print("  speaker_02/")
        print("    utt_0001.wav")
        return
    
    # Find all directories (not just speaker_* prefix)
    speaker_folders = sorted([d for d in data_dir.iterdir() if d.is_dir()])
    
    if len(speaker_folders) == 0:
        print(f"âŒ Error: No speaker folders found in {data_dir}")
        print("Expected folders like: speaker_01, speaker_02, etc.")
        return
    
    print(f"Found {len(speaker_folders)} speakers:")
    
    for speaker_folder in speaker_folders:
        speaker_name = speaker_folder.name
        # Support multiple audio formats
        audio_files = (list(speaker_folder.glob('*.wav')) + 
                      list(speaker_folder.glob('*.mp3')) +
                      list(speaker_folder.glob('*.m4a')) +
                      list(speaker_folder.glob('*.webm')) +
                      list(speaker_folder.glob('*.ogg')))
        
        if len(audio_files) == 0:
            print(f"  âš ï¸  {speaker_name}: No audio files found")
            continue
        
        print(f"  âœ… {speaker_name}: {len(audio_files)} files")
        
        # Her ses dosyasÄ±nÄ± iÅŸle
        for audio_file in audio_files:
            try:
                # YÃ¼kle ve Ã¶n iÅŸle
                audio = processor.load_audio(str(audio_file))
                audio = processor.preprocess_audio(audio)  # 3 saniyeye normalize et
                
                # Ã–zellikleri Ã§Ä±kar (sadece MFCC kullanÄ±lÄ±yor)
                features = processor.extract_mfcc(audio)
                
                # DÃ¼zleÅŸtir (ML modelleri iÃ§in)
                features_flat = features.flatten()
                
                features_list.append(features_flat)
                labels_list.append(speaker_name)
                
            except Exception as e:
                print(f"     âš ï¸  Failed to process {audio_file.name}: {e}")
                continue
    
    if len(features_list) == 0:
        print("\nâŒ Error: No valid audio files found!")
        return
    
    # NumPy dizilerine Ã§evir
    X = np.array(features_list)
    y = np.array(labels_list)
    
    print(f"\nğŸ“Š Dataset Statistics:")
    print(f"   Total samples: {len(X)}")
    print(f"   Features per sample: {X.shape[1]}")
    print(f"   Unique speakers: {len(np.unique(y))}")
    
    # Check if we have at least 2 speakers
    if len(np.unique(y)) < 2:
        print("\nâŒ Error: Need at least 2 different speakers!")
        print("Please add audio files for another speaker before training.")
        print("Model training requires multiple classes.")
        return
    
    # Veriyi bÃ¶l
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nğŸ”¬ Train/Test Split:")
    print(f"   Training samples: {len(X_train)}")
    print(f"   Test samples: {len(X_test)}")
    
    # Cross-validation (eÄŸer istenirse)
    cv_results = None
    if use_cv:
        print(f"\nğŸ”„ Performing {cv_folds}-fold Cross-Validation...")
        base_model = create_model(model_type)
        cv_results = perform_cross_validation(base_model, X_train, y_train, cv_folds)
        print(f"   CV Mean Accuracy: {cv_results['cv_mean']:.4f} ({cv_results['cv_mean']*100:.2f}%)")
        print(f"   CV Std: {cv_results['cv_std']:.4f} ({cv_results['cv_std']*100:.2f}%)")
        print(f"   CV Scores: {[f'{s:.4f}' for s in cv_results['cv_scores']]}")
    
    # Hyperparameter tuning (eÄŸer istenirse)
    best_params = None
    if use_tuning:
        print(f"\nğŸ¯ Performing Hyperparameter Tuning ({tuning_method})...")
        param_grid = get_hyperparameter_grid(model_type)
        
        if not param_grid:
            print(f"   âš ï¸  No hyperparameter grid defined for {model_type}, skipping tuning")
            use_tuning = False
        else:
            base_model = create_model(model_type)
            cv = StratifiedKFold(n_splits=min(5, cv_folds), shuffle=True, random_state=42)
            
            if tuning_method == 'grid':
                search = GridSearchCV(
                    base_model, 
                    param_grid, 
                    cv=cv, 
                    scoring='accuracy',
                    n_jobs=-1,
                    verbose=1
                )
            else:  # random
                search = RandomizedSearchCV(
                    base_model,
                    param_grid,
                    cv=cv,
                    scoring='accuracy',
                    n_iter=n_iter,
                    n_jobs=-1,
                    random_state=42,
                    verbose=1
                )
            
            print(f"   Searching through {len(param_grid)} parameter combinations...")
            search.fit(X_train, y_train)
            best_params = search.best_params_
            model = search.best_estimator_
            
            print(f"   âœ… Best parameters found:")
            for param, value in best_params.items():
                print(f"      {param}: {value}")
            print(f"   Best CV Score: {search.best_score_:.4f} ({search.best_score_*100:.2f}%)")
    
    # Model oluÅŸtur ve eÄŸit (tuning yapÄ±lmadÄ±ysa)
    if not use_tuning:
        print(f"\nğŸ¤– Training {model_names.get(model_type, model_type)} model...")
        model = create_model(model_type)
        model.fit(X_train, y_train)
    
    # DeÄŸerlendirme
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    print(f"\nğŸ“ˆ Model Performance:")
    print(f"   Train Accuracy: {train_score:.4f} ({train_score*100:.2f}%)")
    print(f"   Test Accuracy: {test_score:.4f} ({test_score*100:.2f}%)")
    
    # Test tahminleri
    y_pred = model.predict(X_test)
    
    # DetaylÄ± metrikler hesapla
    
    # Macro average (tÃ¼m sÄ±nÄ±flar iÃ§in ortalama)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
    
    # Weighted average (sÄ±nÄ±f bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re aÄŸÄ±rlÄ±klÄ±)
    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    print(f"\nğŸ“‹ Classification Report:")
    print(classification_report(y_test, y_pred))
    
    print(f"\nğŸ¯ Confusion Matrix:")
    print(cm)
    
    print(f"\nğŸ“Š Detailed Metrics:")
    print(f"   Precision (Macro): {precision_macro:.4f}")
    print(f"   Recall (Macro): {recall_macro:.4f}")
    print(f"   F1-Score (Macro): {f1_macro:.4f}")
    
    # Modeli kaydet
    model_filename = get_model_filename(model_type, feature_type)
    model_path = models_dir / model_filename
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    print(f"\nğŸ’¾ Model saved to: {model_path}")
    
    # Model metadata kaydet (detaylÄ± metrikler ile)
    metadata = {
        'model_type': model_type,
        'feature_type': feature_type,
        'feature_shape': X.shape[1],
        'num_speakers': len(np.unique(y)),
        'test_accuracy': float(test_score),
        'train_accuracy': float(train_score),
        'precision_macro': float(precision_macro),
        'recall_macro': float(recall_macro),
        'f1_macro': float(f1_macro),
        'precision_weighted': float(precision_weighted),
        'recall_weighted': float(recall_weighted),
        'f1_weighted': float(f1_weighted),
        'confusion_matrix': cm.tolist(),  # JSON serializable yapmak iÃ§in
        'speakers': sorted(np.unique(y).tolist())  # KonuÅŸmacÄ± listesi
    }
    
    # Cross-validation sonuÃ§larÄ±nÄ± ekle
    if cv_results:
        metadata['cross_validation'] = cv_results
    
    # Hyperparameter tuning sonuÃ§larÄ±nÄ± ekle
    if best_params:
        metadata['best_hyperparameters'] = best_params
        metadata['hyperparameter_tuning_method'] = tuning_method
    metadata_path = models_dir / f'{model_filename}.meta'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        import json
        json.dump(metadata, f, indent=2)
    print(f"ğŸ“‹ Model metadata saved to: {metadata_path}")
    
    # Speaker labels kaydet
    unique_speakers = sorted(np.unique(y))
    labels_path = models_dir / 'speaker_labels.txt'
    with open(labels_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(unique_speakers))
    print(f"ğŸ“ Speaker labels saved to: {labels_path}")
    
    print("\nâœ… Training complete!")
    print(f"\nNow you can use the model in the backend:")
    print(f"  - Model file: models/{model_filename}")
    print(f"  - Feature type: {feature_type}")
    print(f"  - Labels: models/speaker_labels.txt")
    print(f"\nğŸ’¡ Backend'de modeli yÃ¼klemek iÃ§in:")
    print(f"   model_manager.load_model('{model_filename}', model_type='sklearn')")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='KonuÅŸmacÄ± tanÄ±ma modeli eÄŸitimi')
    parser.add_argument(
        '--model',
        type=str,
        default='svm',
        choices=['svm', 'random_forest', 'neural_network', 'adaboost'],
        help='EÄŸitilecek model tipi (default: svm)'
    )
    parser.add_argument(
        '--feature',
        type=str,
        default='mfcc',
        choices=['mfcc'],  # Mel desteÄŸi kaldÄ±rÄ±ldÄ±
        help='KullanÄ±lacak Ã¶zellik tipi: mfcc (default: mfcc, Mel desteÄŸi kaldÄ±rÄ±ldÄ±)'
    )
    parser.add_argument(
        '--cv',
        action='store_true',
        help='Cross-validation kullan (default: False)'
    )
    parser.add_argument(
        '--cv-folds',
        type=int,
        default=5,
        help='Cross-validation fold sayÄ±sÄ± (default: 5)'
    )
    parser.add_argument(
        '--tune',
        action='store_true',
        help='Hyperparameter tuning kullan (default: False)'
    )
    parser.add_argument(
        '--tuning-method',
        type=str,
        default='grid',
        choices=['grid', 'random'],
        help='Hyperparameter tuning yÃ¶ntemi: grid veya random (default: grid)'
    )
    parser.add_argument(
        '--n-iter',
        type=int,
        default=20,
        help='RandomizedSearchCV iÃ§in iterasyon sayÄ±sÄ± (default: 20)'
    )
    
    args = parser.parse_args()
    train_speaker_model(
        model_type=args.model, 
        feature_type=args.feature,
        use_cv=args.cv,
        cv_folds=args.cv_folds,
        use_tuning=args.tune,
        tuning_method=args.tuning_method,
        n_iter=args.n_iter
    )

